{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uaQLffDjfIFD"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dVKdTv755GKT"
   },
   "source": [
    "Source: https://github.com/huggingface/notebooks/blob/main/examples/language_modeling.ipynb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QlQUGxBrgEft"
   },
   "source": [
    "First, load the data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1731331220600,
     "user": {
      "displayName": "Jochem",
      "userId": "13741125791056547006"
     },
     "user_tz": -60
    },
    "id": "Nw90JQmngKFm",
    "outputId": "65511725-d83d-4198-db52-c85700af1048"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator MultiLabelBinarizer from version 1.1.1 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# load the data from the csv file\n",
    "df = pd.read_csv(\"/content/drive/MyDrive/1Jupyter/SCRIPTIE/data.csv\")\n",
    "\n",
    "# load the mlb files back in, to get the classes and transform functions\n",
    "with open(\"/content/drive/MyDrive/1Jupyter/SCRIPTIE/dom_mlb.pkl\", \"rb\") as f:\n",
    "    dom_mlb = pickle.load(f)\n",
    "\n",
    "with open(\"/content/drive/MyDrive/1Jupyter/SCRIPTIE/sub_mlb.pkl\", \"rb\") as f:\n",
    "    sub_mlb = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "baOvOQvyhwcE"
   },
   "source": [
    "Next, we want to tokenize all elements in X:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4182,
     "status": "ok",
     "timestamp": 1731331224776,
     "user": {
      "displayName": "Jochem",
      "userId": "13741125791056547006"
     },
     "user_tz": -60
    },
    "id": "9E0P9VAJf55_",
    "outputId": "05c23a8f-7097-4c33-9a95-bde57f2b77fe"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (634 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n",
    "df[\"tokenized_text\"] = df[\"text\"].apply(lambda x: tokenizer(x, add_special_tokens=False))\n",
    "print(type(df[\"tokenized_text\"][2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vAfovWCK-Z0v"
   },
   "source": [
    "After tokenizing, we need to split the text up into chunks of 512, so the sentence bert model can generate its embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MFrQteHmyC_Y"
   },
   "outputs": [],
   "source": [
    "def chunk_text(tokens, chunk_size=512):\n",
    "  if len(tokens[\"input_ids\"]) <= chunk_size:\n",
    "    return [tokenizer.decode(tokens[\"input_ids\"])]\n",
    "\n",
    "  chunks = []\n",
    "  for i in range(0, len(tokens[\"input_ids\"]), chunk_size):\n",
    "      chunk = {k: t[i:i + chunk_size] for k, t in tokens.items()}\n",
    "      chunks.append(tokenizer.decode(chunk[\"input_ids\"]))\n",
    "\n",
    "  return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UnoZUXXX29Tb"
   },
   "outputs": [],
   "source": [
    "df[\"chunked_text\"] = df[\"tokenized_text\"].apply(lambda x: chunk_text(x, 512))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UYIp-DdvidQ_"
   },
   "source": [
    "The texts are successfully split into chunks with a maximum length of 512, so now let's generate the embeddings. We used an SBert model for this, but"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u1gipSIyOcQL"
   },
   "outputs": [],
   "source": [
    "# function to generate the embeddings, takes a df with \"chunked_text\" and a bertmodel and returns a pd DF with the embeddings\n",
    "def generate_embeddings(df, bertmodel):\n",
    "  return df[\"chunked_text\"].apply(lambda x: np.average([bertmodel.encode(i) for i in x],\n",
    "                                                        weights = [len(i) for i in x],\n",
    "                                                        axis = 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q8Mkt3i2PCSW"
   },
   "source": [
    "Lets generate some embeddings for a couple different SBert models:\n",
    "https://www.sbert.net/docs/sentence_transformer/pretrained_models.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7426,
     "status": "ok",
     "timestamp": 1731331236442,
     "user": {
      "displayName": "Jochem",
      "userId": "13741125791056547006"
     },
     "user_tz": -60
    },
    "id": "4NgJ8iQkLA4T",
    "outputId": "9f47f333-3155-4bea-f133-38dac95be70c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "# suppresses the loading bars that the models have, to reduce spam\n",
    "original_stdout = sys.stdout\n",
    "sys.stdout = open(os.devnull, 'w')\n",
    "SBERTmodels = {\n",
    "    \"mpnet\": SentenceTransformer(\"all-mpnet-base-v2\"),\n",
    "    \"multiqa\": SentenceTransformer(\"multi-qa-mpnet-base-dot-v1\"),\n",
    "    \"distilroberta\": SentenceTransformer(\"all-distilroberta-v1\"),\n",
    "    \"minilm\": SentenceTransformer(\"all-MiniLM-L12-v2\"),\n",
    "    \"paraphrasemultilang\": SentenceTransformer(\"paraphrase-multilingual-mpnet-base-v2\")\n",
    "}\n",
    "sys.stdout = original_stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ybl0OVR7jgHs"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "\n",
    "# when training on sub categories, some simply do not have enough data in the 600 point dataset... so for now only train it on the dominant categories\n",
    "def logistic_regression_classifier(X_train, y_train, X_test):\n",
    "    lr = MultiOutputClassifier(LogisticRegression(class_weight=\"balanced\", solver= \"liblinear\", max_iter=100))\n",
    "    lr.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = lr.predict(X_test)\n",
    "    return y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 112630,
     "status": "ok",
     "timestamp": 1731334718695,
     "user": {
      "displayName": "Jochem",
      "userId": "13741125791056547006"
     },
     "user_tz": -60
    },
    "id": "_Q9YutKtU3TQ",
    "outputId": "55a8aad5-c66d-4962-d149-ca19bc75771d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mpnet embeddings:\n",
      "                                                        precision    recall  f1-score   support\n",
      "\n",
      "                          CC: Amplifying Climate Fears       0.54      1.00      0.70         7\n",
      "                      CC: Climate change is beneficial       0.00      0.00      0.00         0\n",
      "              CC: Controversy about green technologies       0.20      1.00      0.33         1\n",
      "                     CC: Criticism of climate movement       0.44      1.00      0.62         4\n",
      "                     CC: Criticism of climate policies       0.40      0.80      0.53         5\n",
      "         CC: Criticism of institutions and authorities       0.40      1.00      0.57         6\n",
      "                        CC: Downplaying climate change       0.22      1.00      0.36         2\n",
      "       CC: Green policies are geopolitical instruments       0.00      0.00      0.00         0\n",
      " CC: Hidden plots by secret schemes of powerful groups       0.29      0.67      0.40         3\n",
      "          CC: Questioning the measurements and science       0.00      0.00      0.00         0\n",
      "                                                 Other       0.50      0.64      0.56        14\n",
      "                     URW: Amplifying war-related fears       0.56      1.00      0.71        10\n",
      "URW: Blaming the war on others rather than the invader       0.16      1.00      0.28         4\n",
      "                             URW: Discrediting Ukraine       0.34      1.00      0.51        10\n",
      "                 URW: Discrediting the West, Diplomacy       0.59      0.81      0.68        16\n",
      "                           URW: Distrust towards Media       0.11      0.50      0.18         2\n",
      "URW: Hidden plots by secret schemes of powerful groups       0.00      0.00      0.00         1\n",
      "               URW: Negative Consequences for the West       0.23      0.75      0.35         4\n",
      "                            URW: Overpraising the West       0.00      0.00      0.00         1\n",
      "                                 URW: Praise of Russia       0.31      0.73      0.43        11\n",
      "                             URW: Russia is the Victim       0.22      0.62      0.32         8\n",
      "                         URW: Speculating war outcomes       0.13      0.67      0.22         3\n",
      "\n",
      "                                             micro avg       0.31      0.81      0.44       112\n",
      "                                             macro avg       0.26      0.65      0.35       112\n",
      "                                          weighted avg       0.39      0.81      0.52       112\n",
      "                                           samples avg       0.34      0.80      0.44       112\n",
      "\n",
      "----------------------------------------------------------------------------------\n",
      "multiqa embeddings:\n",
      "                                                        precision    recall  f1-score   support\n",
      "\n",
      "                          CC: Amplifying Climate Fears       0.83      0.71      0.77         7\n",
      "                      CC: Climate change is beneficial       0.00      0.00      0.00         0\n",
      "              CC: Controversy about green technologies       0.50      1.00      0.67         1\n",
      "                     CC: Criticism of climate movement       0.60      0.75      0.67         4\n",
      "                     CC: Criticism of climate policies       0.44      0.80      0.57         5\n",
      "         CC: Criticism of institutions and authorities       0.50      0.67      0.57         6\n",
      "                        CC: Downplaying climate change       0.17      0.50      0.25         2\n",
      "       CC: Green policies are geopolitical instruments       0.00      0.00      0.00         0\n",
      " CC: Hidden plots by secret schemes of powerful groups       1.00      0.33      0.50         3\n",
      "          CC: Questioning the measurements and science       0.00      0.00      0.00         0\n",
      "                                                 Other       0.65      0.79      0.71        14\n",
      "                     URW: Amplifying war-related fears       0.75      0.60      0.67        10\n",
      "URW: Blaming the war on others rather than the invader       0.30      0.75      0.43         4\n",
      "                             URW: Discrediting Ukraine       0.35      0.70      0.47        10\n",
      "                 URW: Discrediting the West, Diplomacy       0.65      0.69      0.67        16\n",
      "                           URW: Distrust towards Media       0.00      0.00      0.00         2\n",
      "URW: Hidden plots by secret schemes of powerful groups       0.00      0.00      0.00         1\n",
      "               URW: Negative Consequences for the West       0.29      0.50      0.36         4\n",
      "                            URW: Overpraising the West       0.00      0.00      0.00         1\n",
      "                                 URW: Praise of Russia       0.40      0.55      0.46        11\n",
      "                             URW: Russia is the Victim       0.50      0.50      0.50         8\n",
      "                         URW: Speculating war outcomes       0.00      0.00      0.00         3\n",
      "\n",
      "                                             micro avg       0.46      0.62      0.53       112\n",
      "                                             macro avg       0.36      0.45      0.38       112\n",
      "                                          weighted avg       0.52      0.62      0.55       112\n",
      "                                           samples avg       0.46      0.64      0.51       112\n",
      "\n",
      "----------------------------------------------------------------------------------\n",
      "distilroberta embeddings:\n",
      "                                                        precision    recall  f1-score   support\n",
      "\n",
      "                          CC: Amplifying Climate Fears       0.47      1.00      0.64         7\n",
      "                      CC: Climate change is beneficial       0.00      0.00      0.00         0\n",
      "              CC: Controversy about green technologies       0.17      1.00      0.29         1\n",
      "                     CC: Criticism of climate movement       0.36      1.00      0.53         4\n",
      "                     CC: Criticism of climate policies       0.42      1.00      0.59         5\n",
      "         CC: Criticism of institutions and authorities       0.38      0.83      0.53         6\n",
      "                        CC: Downplaying climate change       0.20      1.00      0.33         2\n",
      "       CC: Green policies are geopolitical instruments       0.00      0.00      0.00         0\n",
      " CC: Hidden plots by secret schemes of powerful groups       0.40      0.67      0.50         3\n",
      "          CC: Questioning the measurements and science       0.00      0.00      0.00         0\n",
      "                                                 Other       0.48      0.79      0.59        14\n",
      "                     URW: Amplifying war-related fears       0.56      1.00      0.71        10\n",
      "URW: Blaming the war on others rather than the invader       0.14      1.00      0.25         4\n",
      "                             URW: Discrediting Ukraine       0.30      1.00      0.47        10\n",
      "                 URW: Discrediting the West, Diplomacy       0.59      0.81      0.68        16\n",
      "                           URW: Distrust towards Media       0.10      0.50      0.17         2\n",
      "URW: Hidden plots by secret schemes of powerful groups       0.00      0.00      0.00         1\n",
      "               URW: Negative Consequences for the West       0.18      0.50      0.27         4\n",
      "                            URW: Overpraising the West       0.00      0.00      0.00         1\n",
      "                                 URW: Praise of Russia       0.36      0.91      0.51        11\n",
      "                             URW: Russia is the Victim       0.26      0.62      0.37         8\n",
      "                         URW: Speculating war outcomes       0.00      0.00      0.00         3\n",
      "\n",
      "                                             micro avg       0.30      0.82      0.44       112\n",
      "                                             macro avg       0.24      0.62      0.34       112\n",
      "                                          weighted avg       0.39      0.82      0.51       112\n",
      "                                           samples avg       0.32      0.82      0.44       112\n",
      "\n",
      "----------------------------------------------------------------------------------\n",
      "minilm embeddings:\n",
      "                                                        precision    recall  f1-score   support\n",
      "\n",
      "                          CC: Amplifying Climate Fears       0.54      1.00      0.70         7\n",
      "                      CC: Climate change is beneficial       0.00      0.00      0.00         0\n",
      "              CC: Controversy about green technologies       0.12      1.00      0.22         1\n",
      "                     CC: Criticism of climate movement       0.33      1.00      0.50         4\n",
      "                     CC: Criticism of climate policies       0.29      0.80      0.42         5\n",
      "         CC: Criticism of institutions and authorities       0.38      1.00      0.55         6\n",
      "                        CC: Downplaying climate change       0.10      0.50      0.17         2\n",
      "       CC: Green policies are geopolitical instruments       0.00      0.00      0.00         0\n",
      " CC: Hidden plots by secret schemes of powerful groups       0.38      1.00      0.55         3\n",
      "          CC: Questioning the measurements and science       0.00      0.00      0.00         0\n",
      "                                                 Other       0.43      0.71      0.54        14\n",
      "                     URW: Amplifying war-related fears       0.47      0.90      0.62        10\n",
      "URW: Blaming the war on others rather than the invader       0.17      1.00      0.30         4\n",
      "                             URW: Discrediting Ukraine       0.32      0.90      0.47        10\n",
      "                 URW: Discrediting the West, Diplomacy       0.62      0.94      0.75        16\n",
      "                           URW: Distrust towards Media       0.20      0.50      0.29         2\n",
      "URW: Hidden plots by secret schemes of powerful groups       0.00      0.00      0.00         1\n",
      "               URW: Negative Consequences for the West       0.20      0.75      0.32         4\n",
      "                            URW: Overpraising the West       0.00      0.00      0.00         1\n",
      "                                 URW: Praise of Russia       0.36      0.82      0.50        11\n",
      "                             URW: Russia is the Victim       0.24      0.62      0.34         8\n",
      "                         URW: Speculating war outcomes       0.07      0.33      0.12         3\n",
      "\n",
      "                                             micro avg       0.30      0.82      0.44       112\n",
      "                                             macro avg       0.24      0.63      0.33       112\n",
      "                                          weighted avg       0.38      0.82      0.51       112\n",
      "                                           samples avg       0.34      0.80      0.44       112\n",
      "\n",
      "----------------------------------------------------------------------------------\n",
      "paraphrasemultilang embeddings:\n",
      "                                                        precision    recall  f1-score   support\n",
      "\n",
      "                          CC: Amplifying Climate Fears       0.67      0.86      0.75         7\n",
      "                      CC: Climate change is beneficial       0.00      0.00      0.00         0\n",
      "              CC: Controversy about green technologies       0.17      1.00      0.29         1\n",
      "                     CC: Criticism of climate movement       0.33      0.75      0.46         4\n",
      "                     CC: Criticism of climate policies       0.33      0.80      0.47         5\n",
      "         CC: Criticism of institutions and authorities       0.36      0.83      0.50         6\n",
      "                        CC: Downplaying climate change       0.20      0.50      0.29         2\n",
      "       CC: Green policies are geopolitical instruments       0.00      0.00      0.00         0\n",
      " CC: Hidden plots by secret schemes of powerful groups       0.75      1.00      0.86         3\n",
      "          CC: Questioning the measurements and science       0.00      0.00      0.00         0\n",
      "                                                 Other       0.53      0.64      0.58        14\n",
      "                     URW: Amplifying war-related fears       0.50      0.60      0.55        10\n",
      "URW: Blaming the war on others rather than the invader       0.16      0.75      0.26         4\n",
      "                             URW: Discrediting Ukraine       0.32      0.70      0.44        10\n",
      "                 URW: Discrediting the West, Diplomacy       0.59      0.81      0.68        16\n",
      "                           URW: Distrust towards Media       0.14      0.50      0.22         2\n",
      "URW: Hidden plots by secret schemes of powerful groups       0.00      0.00      0.00         1\n",
      "               URW: Negative Consequences for the West       0.25      0.50      0.33         4\n",
      "                            URW: Overpraising the West       0.00      0.00      0.00         1\n",
      "                                 URW: Praise of Russia       0.44      0.73      0.55        11\n",
      "                             URW: Russia is the Victim       0.27      0.50      0.35         8\n",
      "                         URW: Speculating war outcomes       0.12      0.33      0.18         3\n",
      "\n",
      "                                             micro avg       0.35      0.69      0.46       112\n",
      "                                             macro avg       0.28      0.54      0.35       112\n",
      "                                          weighted avg       0.42      0.69      0.51       112\n",
      "                                           samples avg       0.37      0.68      0.44       112\n",
      "\n",
      "----------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# this code generates the dataframe of embeddings. Per document, it encodes every chunk, and averages it into the final embedding by the number of words per chunk\n",
    "f1_scores =  {}\n",
    "embeddings = {}\n",
    "\n",
    "for model_name, model in SBERTmodels.items():\n",
    "  embeddings[model_name] = generate_embeddings(df, model)\n",
    "\n",
    "  X = np.vstack(embeddings[model_name].values)\n",
    "  y = df[dom_mlb.classes_].values\n",
    "\n",
    "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=1)\n",
    "  y_pred = logistic_regression_classifier(X_train=X_train,y_train=y_train,X_test=X_test)\n",
    "\n",
    "  class_report = classification_report(y_true=y_test,y_pred=y_pred,target_names=dom_mlb.classes_, output_dict=True, zero_division=0.0)\n",
    "\n",
    "  print(f\"{model_name} embeddings:\")\n",
    "  print(classification_report(y_true=y_test,y_pred=y_pred,target_names=dom_mlb.classes_, zero_division=0.0))\n",
    "  print(\"----------------------------------------------------------------------------------\")\n",
    "\n",
    "  f1_scores[model_name] = class_report[\"samples avg\"]['f1-score']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fiIC8JC0sptX"
   },
   "source": [
    "Not amazing, but at least we now know the embedding is working as it should!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NfeX3TJwhpkr"
   },
   "source": [
    "Lastly, lets save the best embeddings now that we have them. The embeddings are saved as a pickle file. to do this, we take the max of the f1 sample scores, for which we want to optimize the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1731333954453,
     "user": {
      "displayName": "Jochem",
      "userId": "13741125791056547006"
     },
     "user_tz": -60
    },
    "id": "VTBNLxIzl3Rx",
    "outputId": "5f50faa4-eb76-4be2-e264-94c147ad87b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      [0.15851294905673272, -0.1362919395903227, -0....\n",
      "1      [-0.02812150865793228, 0.1337200105190277, -0....\n",
      "2      [0.0011932615172948427, -0.039664918359867324,...\n",
      "3      [0.07796116809005393, -0.19448494981831407, -0...\n",
      "4      [-0.09016437083482742, -0.11372177302837372, -...\n",
      "                             ...                        \n",
      "621    [0.06095129996538162, -0.06814509630203247, -0...\n",
      "622    [-0.061890374807230496, 0.07734311149534533, -...\n",
      "623    [-0.04900439828634262, -0.10150399804115295, -...\n",
      "624    [0.017338981528675284, 0.07199527126815737, -0...\n",
      "625    [0.04894140362739563, -0.12287387251853943, -0...\n",
      "Name: chunked_text, Length: 626, dtype: object\n"
     ]
    }
   ],
   "source": [
    "bestembeddings = embeddings[max(f1_scores)]\n",
    "print(bestembeddings)\n",
    "\n",
    "with open(\"/content/drive/MyDrive/1Jupyter/SCRIPTIE/embeddings.pkl\", \"wb\") as f:\n",
    "    pickle.dump(bestembeddings, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YypPygZsc48T"
   },
   "source": [
    "Use the following code to open up the embedding file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1731333986620,
     "user": {
      "displayName": "Jochem",
      "userId": "13741125791056547006"
     },
     "user_tz": -60
    },
    "id": "s3jztwUZoLqM",
    "outputId": "f71e8723-744b-4b95-9e45-e088c4bfd8a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      [0.15851294905673272, -0.1362919395903227, -0....\n",
      "1      [-0.02812150865793228, 0.1337200105190277, -0....\n",
      "2      [0.0011932615172948427, -0.039664918359867324,...\n",
      "3      [0.07796116809005393, -0.19448494981831407, -0...\n",
      "4      [-0.09016437083482742, -0.11372177302837372, -...\n",
      "                             ...                        \n",
      "621    [0.06095129996538162, -0.06814509630203247, -0...\n",
      "622    [-0.061890374807230496, 0.07734311149534533, -...\n",
      "623    [-0.04900439828634262, -0.10150399804115295, -...\n",
      "624    [0.017338981528675284, 0.07199527126815737, -0...\n",
      "625    [0.04894140362739563, -0.12287387251853943, -0...\n",
      "Name: chunked_text, Length: 626, dtype: object\n"
     ]
    }
   ],
   "source": [
    "with open(\"/content/drive/MyDrive/1Jupyter/SCRIPTIE/embeddings.pkl\", \"rb\") as f:\n",
    "    bestembeddings = pickle.load(f)\n",
    "\n",
    "print(bestembeddings)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMUqDKmo1z15wupuNpc0myz",
   "gpuType": "T4",
   "mount_file_id": "1wkU-lbZmZzlMQjKaUCOtpd94is0y1abu",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
