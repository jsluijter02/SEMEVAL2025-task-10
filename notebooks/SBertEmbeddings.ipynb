{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "uaQLffDjfIFD"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dVKdTv755GKT"
   },
   "source": [
    "Source: https://github.com/huggingface/notebooks/blob/main/examples/language_modeling.ipynb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QlQUGxBrgEft"
   },
   "source": [
    "First, load the data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1731331220600,
     "user": {
      "displayName": "Jochem",
      "userId": "13741125791056547006"
     },
     "user_tz": -60
    },
    "id": "Nw90JQmngKFm",
    "outputId": "65511725-d83d-4198-db52-c85700af1048"
   },
   "outputs": [],
   "source": [
    "# load the data from the csv file\n",
    "df = pd.read_csv(\"../data/newdata.csv\")\n",
    "\n",
    "# load the mlb files back in, to get the classes and transform functions\n",
    "with open(\"../pkl_files/dom_mlb.pkl\", \"rb\") as f:\n",
    "    dom_mlb = pickle.load(f)\n",
    "\n",
    "with open(\"../pkl_files/sub_mlb.pkl\", \"rb\") as f:\n",
    "    sub_mlb = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "baOvOQvyhwcE"
   },
   "source": [
    "Next, we want to tokenize all elements in X:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4182,
     "status": "ok",
     "timestamp": 1731331224776,
     "user": {
      "displayName": "Jochem",
      "userId": "13741125791056547006"
     },
     "user_tz": -60
    },
    "id": "9E0P9VAJf55_",
    "outputId": "05c23a8f-7097-4c33-9a95-bde57f2b77fe"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72ff761745994039b352127949c5dba9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "516ba9b262554362a87256e72206e4b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "648ecc9858e04819839a14733b27a6a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e38df3db54440adaeaab9542888f5cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (634 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n",
    "df[\"tokenized_text\"] = df[\"text\"].apply(lambda x: tokenizer(x, add_special_tokens=False))\n",
    "print(type(df[\"tokenized_text\"][2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vAfovWCK-Z0v"
   },
   "source": [
    "After tokenizing, we need to split the text up into chunks of 512, so the sentence bert model can generate its embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "MFrQteHmyC_Y"
   },
   "outputs": [],
   "source": [
    "def chunk_text(tokens, chunk_size=512):\n",
    "  if len(tokens[\"input_ids\"]) <= chunk_size:\n",
    "    return [tokenizer.decode(tokens[\"input_ids\"])]\n",
    "\n",
    "  chunks = []\n",
    "  for i in range(0, len(tokens[\"input_ids\"]), chunk_size):\n",
    "      chunk = {k: t[i:i + chunk_size] for k, t in tokens.items()}\n",
    "      chunks.append(tokenizer.decode(chunk[\"input_ids\"]))\n",
    "\n",
    "  return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "UnoZUXXX29Tb"
   },
   "outputs": [],
   "source": [
    "df[\"chunked_text\"] = df[\"tokenized_text\"].apply(lambda x: chunk_text(x, 512))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UYIp-DdvidQ_"
   },
   "source": [
    "The texts are successfully split into chunks with a maximum length of 512, so now let's generate the embeddings. We used an SBert model for this, but"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "u1gipSIyOcQL"
   },
   "outputs": [],
   "source": [
    "# function to generate the embeddings, takes a df with \"chunked_text\" and a bertmodel and returns a pd DF with the embeddings\n",
    "def generate_embeddings(df, bertmodel):\n",
    "  return df[\"chunked_text\"].apply(lambda x: np.average([bertmodel.encode(i) for i in x],\n",
    "                                                        weights = [len(i) for i in x],\n",
    "                                                        axis = 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q8Mkt3i2PCSW"
   },
   "source": [
    "Lets generate some embeddings for a couple different SBert models:\n",
    "https://www.sbert.net/docs/sentence_transformer/pretrained_models.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7426,
     "status": "ok",
     "timestamp": 1731331236442,
     "user": {
      "displayName": "Jochem",
      "userId": "13741125791056547006"
     },
     "user_tz": -60
    },
    "id": "4NgJ8iQkLA4T",
    "outputId": "9f47f333-3155-4bea-f133-38dac95be70c"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bc0ee94eee64a7f9f70c69270454059",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:  44%|####4     | 493M/1.11G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0796470ada404e37a6346878d5b3921d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/402 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dd4d39e6f334747a97cbfc54d0a1e87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c79cdd028fbf4af0b7b2a439e1dcbe08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.08M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f676ec1a730a417ba2c41b697e2d60f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51f73993c3db47b481bdcb607712df3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "# suppresses the loading bars that the models have, to reduce spam\n",
    "original_stdout = sys.stdout\n",
    "sys.stdout = open(os.devnull, 'w')\n",
    "SBERTmodels = {\n",
    "    \"mpnet\": SentenceTransformer(\"all-mpnet-base-v2\"),\n",
    "    \"multiqa\": SentenceTransformer(\"multi-qa-mpnet-base-dot-v1\"),\n",
    "    \"distilroberta\": SentenceTransformer(\"all-distilroberta-v1\"),\n",
    "    \"minilm\": SentenceTransformer(\"all-MiniLM-L12-v2\"),\n",
    "    \"paraphrasemultilang\": SentenceTransformer(\"paraphrase-multilingual-mpnet-base-v2\")\n",
    "}\n",
    "sys.stdout = original_stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "Ybl0OVR7jgHs"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "\n",
    "# when training on sub categories, some simply do not have enough data in the 600 point dataset... so for now only train it on the dominant categories\n",
    "def logistic_regression_classifier(X_train, y_train, X_test):\n",
    "    lr = MultiOutputClassifier(LogisticRegression(class_weight=\"balanced\", solver= \"liblinear\", max_iter=100))\n",
    "    lr.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = lr.predict(X_test)\n",
    "    return y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 112630,
     "status": "ok",
     "timestamp": 1731334718695,
     "user": {
      "displayName": "Jochem",
      "userId": "13741125791056547006"
     },
     "user_tz": -60
    },
    "id": "_Q9YutKtU3TQ",
    "outputId": "55a8aad5-c66d-4962-d149-ca19bc75771d"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# this code generates the dataframe of embeddings. Per document, it encodes every chunk, and averages it into the final embedding by the number of words per chunk\n",
    "f1_scores =  {}\n",
    "embeddings = {}\n",
    "\n",
    "for model_name, model in SBERTmodels.items():\n",
    "  embeddings[model_name] = generate_embeddings(df, model)\n",
    "\n",
    "  X = np.vstack(embeddings[model_name].values)\n",
    "  y = df[dom_mlb.classes_].values\n",
    "\n",
    "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=1)\n",
    "  y_pred = logistic_regression_classifier(X_train=X_train,y_train=y_train,X_test=X_test)\n",
    "\n",
    "  class_report = classification_report(y_true=y_test,y_pred=y_pred,target_names=dom_mlb.classes_, output_dict=True, zero_division=0.0)\n",
    "\n",
    "  print(f\"{model_name} embeddings:\")\n",
    "  print(classification_report(y_true=y_test,y_pred=y_pred,target_names=dom_mlb.classes_, zero_division=0.0))\n",
    "  print(\"----------------------------------------------------------------------------------\")\n",
    "\n",
    "  f1_scores[model_name] = class_report[\"samples avg\"]['f1-score']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fiIC8JC0sptX"
   },
   "source": [
    "Not amazing, but at least we now know the embedding is working as it should!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NfeX3TJwhpkr"
   },
   "source": [
    "Lastly, lets save the best embeddings now that we have them. The embeddings are saved as a pickle file. to do this, we take the max of the f1 sample scores, for which we want to optimize the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1731333954453,
     "user": {
      "displayName": "Jochem",
      "userId": "13741125791056547006"
     },
     "user_tz": -60
    },
    "id": "VTBNLxIzl3Rx",
    "outputId": "5f50faa4-eb76-4be2-e264-94c147ad87b1"
   },
   "outputs": [],
   "source": [
    "bestembeddings = embeddings[max(f1_scores)]\n",
    "print(bestembeddings)\n",
    "\n",
    "with open(\"../pkl_files/embeddings.pkl\", \"wb\") as f:\n",
    "    pickle.dump(bestembeddings, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YypPygZsc48T"
   },
   "source": [
    "Use the following code to open up the embedding file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1731333986620,
     "user": {
      "displayName": "Jochem",
      "userId": "13741125791056547006"
     },
     "user_tz": -60
    },
    "id": "s3jztwUZoLqM",
    "outputId": "f71e8723-744b-4b95-9e45-e088c4bfd8a7"
   },
   "outputs": [],
   "source": [
    "with open(\"../pkl_files/embeddings.pkl\", \"rb\") as f:\n",
    "    bestembeddings = pickle.load(f)\n",
    "\n",
    "print(bestembeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMUqDKmo1z15wupuNpc0myz",
   "gpuType": "T4",
   "mount_file_id": "1wkU-lbZmZzlMQjKaUCOtpd94is0y1abu",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
